This is the final phase of the LLMOps lifecycle. You have built, trained, tested, and deployed a custom model. But in production, data drifts, edge cases emerge, and models hallucinate.

In Phase 6, we establish a feedback loop. We will wrap your deployed model in an observability layer so you can see exactly _what_ the model is outputting in real-time, how long it takes, and whether its SQL queries are crashing the database.

Here is your step-by-step guide for **Phase 6: Monitoring & Observability**.

| Section            | Details                                                                                                                                                                                                             |
| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Phase Name**     | **Phase 6: Monitoring (Tracing & Error Tracking)**                                                                                                                                                                  |
| **Description**    | This phase provides "X-ray vision" for your production application. It tracks every user request, the exact SQL generated by your model, and whether that SQL executed successfully or failed due to syntax errors. |
| **Key Activities** | • Deploying an observability server (Arize Phoenix).<br>                                                                                                                                                            |

<br>• Instrumenting your LangChain application to automatically send traces.<br>

<br>• Monitoring production traffic to identify bad outputs.<br>

<br>• Catching database syntax errors linked to specific LLM generations. |
| **Tools** | • **Arize Phoenix:** An open-source AI observability platform that visualizes traces, token usage, and latency.<br>

<br>• **OpenInference:** The open standard for instrumenting LLM applications (used to connect LangChain to Phoenix). |
| **Prerequisites** | The deployed `qwen-sql-prod` model running in your Ollama container from Phase 5. |
| **Tips & Best Practices** | • **Track the End-to-End Flow:** Don't just log the LLM response. Log the user's initial question _and_ the database error message so you know exactly why the AI failed.<br>

<br>• **Identify Drift:** Over time, monitor if users start asking about "2025 sales" instead of "2024 sales". If your schema changed, your model will need retraining. |
| **Learning Resources** | **[Arize Phoenix Documentation](https://docs.arize.com/phoenix/)** – Official guides on analyzing trace graphs and spans. |

---

### **Step 1: Update the Docker Architecture**

We need to add the Arize Phoenix server to our stack to collect and visualize the telemetry data.

1. **Update `docker-compose.yml**`:
Add the `phoenix` service to your existing file.

```yaml
services:
  # ... (Keep ollama-service, mlflow-server, unsloth-trainer, and postgres-db)

  # Service 6: Arize Phoenix (Observability UI and Collector)
  phoenix:
    image: arizephoenix/phoenix:latest
    container_name: sql_phoenix
    ports:
      - "6006:6006" # UI & HTTP Tracing Collector
      - "4317:4317" # gRPC Tracing Collector
```

2. **Update `requirements.txt**`:
Add the instrumentation libraries to your `lab-runner` environment.

```text
langchain
langchain-community
langchain-ollama
psycopg2-binary
arize-phoenix
openinference-instrumentation-langchain

```

3. **Rebuild and Start**:

```bash
docker compose up -d --build

```

You can now access the Phoenix UI by opening `http://localhost:6006` in your browser.

---

### **Step 2: The Production Application Gateway**

We will write a simulated production script. This script acts as your backend server: it receives a user question, asks the AI for SQL, tries to execute the SQL against Postgres, and returns the result.

Crucially, we will **instrument** this script so that everything happening under the hood is sent to Phoenix.

Create `src/production_app.py`:

````python
import os
import psycopg2
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from phoenix.otel import register
from openinference.instrumentation.langchain import LangChainInstrumentor

# 1. SETUP MLOPS TRACING
# This connects LangChain to our local Dockerized Phoenix server
print("Connecting to Arize Phoenix...")
tracer_provider = register(
    project_name="text-to-sql-production",
    endpoint="http://phoenix:6006/v1/traces"
)
# Automatically capture all LangChain calls
LangChainInstrumentor().instrument(tracer_provider=tracer_provider)

# 2. INITIALIZE PRODUCTION MODEL
# Notice we don't need the system prompt here; we baked it into the Modelfile in Phase 5!
llm = ChatOllama(
    base_url="http://ollama-service:11434",
    model="qwen-sql-prod",
    temperature=0
)
prompt = ChatPromptTemplate.from_template("Question: {question}")
chain = prompt | llm

DB_URI = os.getenv("DB_URI", "postgresql://testuser:testpass@postgres-db:5432/company_db")

def handle_user_request(question: str):
    print(f"\nUser asked: '{question}'")

    # A. Generate SQL (This step is automatically traced by Phoenix)
    response = chain.invoke({"question": question})
    sql_query = response.content.replace("```sql", "").replace("```", "").strip()

    print(f"Generated SQL: {sql_query}")

    # B. Execute against the database
    try:
        conn = psycopg2.connect(DB_URI)
        cur = conn.cursor()
        cur.execute(sql_query)
        results = cur.fetchall()
        cur.close()
        conn.close()
        print(f"✅ DB Success! Rows returned: {len(results)}")
        return results
    except Exception as e:
        # C. Catch and report syntax errors
        print(f"❌ DB Error: {e}")
        return str(e)

if __name__ == "__main__":
    # Simulate a successful query
    handle_user_request("Show me the total revenue for the SMB tier.")

    # Simulate an edge-case query that might break the model
    # (e.g., asking for a column 'employee_count' that doesn't exist)
    handle_user_request("How many employees are in the NA region?")

````

---

### **Step 3: Execute and Monitor**

Run your production simulation from inside the lab runner:

```bash
docker exec sql_lab_runner python src/production_app.py

```

**The "Ops" Verification:**

1. Open your browser and go to `http://localhost:6006`.
2. Click on the **Projects** tab and select `text-to-sql-production`.
3. You will see a list of traces (one for every time `handle_user_request` was called).
4. Click on a trace to see the **Span details**.

- You can see the exact input, the exact output, how many milliseconds inference took, and the token count.

**The Feedback Loop:**
If a user asks for a column that doesn't exist, the database will throw an error (which prints to your terminal). As an AI Engineer, your workflow is now:

1. Identify the failing query.
2. Check Phoenix to see what SQL the model generated.
3. Add that specific failure scenario to your synthetic dataset in Phase 2.
4. Retrain the model in Phase 3.

**Congratulations!** You have completely executed the full LLMOps lifecycle inside a local Dockerized lab. You moved from model selection, to data tracking, fine-tuning with LoRA, automated execution evaluation, GGUF deployment, and finally, production tracing.

Would you like advice on how to structure this entire end-to-end architecture as a portfolio repository for your resume?

[Understanding Tracing and Instrumentation with Arize Phoenix](https://www.youtube.com/watch?v=j5WwaknZVDY)
This tutorial provides a practical walkthrough of how Phoenix captures spans and tracks errors within LangChain workflows.
